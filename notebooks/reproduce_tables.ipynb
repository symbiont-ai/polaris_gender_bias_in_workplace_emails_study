{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Tables from \"Disentangling Generation and LLM-Judge Effects in Workplace Emails: Gender-Coded Differences Across Models\"\n\nThis notebook reproduces all tables from the paper using the raw data.\n\n**Key methodological note:** The unit of analysis is the *persona* (n=30 per gender), not individual emails. Each persona generates 3 emails, which are averaged before statistical testing. This respects the clustering structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data/raw')\n",
    "\n",
    "def load_emails(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_ratings(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Fix parse errors\n",
    "    for r in data:\n",
    "        if r.get('parse_error') and r.get('raw_response'):\n",
    "            raw = r['raw_response']\n",
    "            for field in ['likelihood_to_grant_raise', 'professionalism', \n",
    "                         'perceived_confidence', 'perceived_competence',\n",
    "                         'likelihood_to_send_correction', 'perceived_reasonableness', \n",
    "                         'seems_entitled']:\n",
    "                if field not in r:\n",
    "                    match = re.search(rf'\"{field}\":\\s*(\\d)', raw)\n",
    "                    if match:\n",
    "                        r[field] = int(match.group(1))\n",
    "    return data\n",
    "\n",
    "# Load emails\n",
    "gpt_emails = load_emails(DATA_DIR / 'emails_gpt52.json')\n",
    "gemini_emails = load_emails(DATA_DIR / 'emails_gemini.json')\n",
    "\n",
    "# Load ratings\n",
    "ratings = {\n",
    "    'gemini_nat': load_ratings(DATA_DIR / 'ratings_gemini_naturalistic.json'),\n",
    "    'gemini_deb': load_ratings(DATA_DIR / 'ratings_gemini_debiased.json'),\n",
    "    'gemini_blind': load_ratings(DATA_DIR / 'ratings_gemini_blinded.json'),\n",
    "    'gpt52_nat': load_ratings(DATA_DIR / 'ratings_gpt52_naturalistic.json'),\n",
    "    'gpt52_deb': load_ratings(DATA_DIR / 'ratings_gpt52_debiased.json'),\n",
    "    'gpt52_blind': load_ratings(DATA_DIR / 'ratings_gpt52_blinded.json'),\n",
    "}\n",
    "\n",
    "print(f\"GPT-5.2 emails: {len(gpt_emails)}\")\n",
    "print(f\"Gemini emails: {len(gemini_emails)}\")\n",
    "print(f\"Total ratings: {sum(len(v) for v in ratings.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_persona_pattern(emails, scenario, pattern, flags=re.IGNORECASE):\n",
    "    \"\"\"Calculate pattern frequency aggregated to persona level.\"\"\"\n",
    "    subset = [e for e in emails if e['scenario_id'] == scenario and e.get('email_text')]\n",
    "    \n",
    "    f_props = defaultdict(list)\n",
    "    m_props = defaultdict(list)\n",
    "    \n",
    "    for e in subset:\n",
    "        has_pattern = 1 if re.search(pattern, e['email_text'], flags) else 0\n",
    "        if e['gender'] == 'F':\n",
    "            f_props[e['persona_id']].append(has_pattern)\n",
    "        else:\n",
    "            m_props[e['persona_id']].append(has_pattern)\n",
    "    \n",
    "    f_means = [np.mean(v) for v in f_props.values()]\n",
    "    m_means = [np.mean(v) for v in m_props.values()]\n",
    "    \n",
    "    f_mean = np.mean(f_means)\n",
    "    m_mean = np.mean(m_means)\n",
    "    \n",
    "    _, p = stats.mannwhitneyu(f_means, m_means, alternative='two-sided')\n",
    "    \n",
    "    pooled_std = np.sqrt((np.std(f_means, ddof=1)**2 + np.std(m_means, ddof=1)**2) / 2)\n",
    "    d = (f_mean - m_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    return {'f_pct': f_mean * 100, 'm_pct': m_mean * 100, 'p': p, 'd': d}\n",
    "\n",
    "\n",
    "def calc_persona_rating(data, scenario, measure):\n",
    "    \"\"\"Calculate rating difference aggregated to persona level.\"\"\"\n",
    "    subset = [r for r in data if r['scenario_id'] == scenario]\n",
    "    \n",
    "    f_vals = defaultdict(list)\n",
    "    m_vals = defaultdict(list)\n",
    "    \n",
    "    for r in subset:\n",
    "        val = r.get(measure)\n",
    "        if val is not None:\n",
    "            if r['gender'] == 'F':\n",
    "                f_vals[r['persona_id']].append(val)\n",
    "            else:\n",
    "                m_vals[r['persona_id']].append(val)\n",
    "    \n",
    "    f_means = [np.mean(v) for v in f_vals.values()]\n",
    "    m_means = [np.mean(v) for v in m_vals.values()]\n",
    "    \n",
    "    f_mean = np.mean(f_means)\n",
    "    m_mean = np.mean(m_means)\n",
    "    diff = f_mean - m_mean\n",
    "    \n",
    "    _, p = stats.mannwhitneyu(f_means, m_means, alternative='two-sided')\n",
    "    \n",
    "    pooled_std = np.sqrt((np.std(f_means, ddof=1)**2 + np.std(m_means, ddof=1)**2) / 2)\n",
    "    d = diff / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    return {'f_mean': f_mean, 'm_mean': m_mean, 'diff': diff, 'p': p, 'd': d}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2: Generation Patterns\n",
    "\n",
    "Shows linguistic patterns that differ significantly by gender after FDR correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    ('Gemini 2.0', 'S01', 'I believe', r'\\bi believe\\b', re.IGNORECASE),\n",
    "    ('GPT-5.2', 'S02', 'clarify', r'\\bclarify\\b', re.IGNORECASE),\n",
    "    ('Gemini 2.0', 'S02', 'follow-up', r'follow.?up', re.IGNORECASE),\n",
    "    ('GPT-5.2', 'S02', 'wanted to', r'wanted to', re.IGNORECASE),\n",
    "    ('GPT-5.2', 'S02', 'full name sig', r'\\n[A-Z][a-z]+ [A-Z][a-z]+\\s*$', 0),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model, scenario, name, pattern, flags in patterns:\n",
    "    emails = gpt_emails if model == 'GPT-5.2' else gemini_emails\n",
    "    res = calc_persona_pattern(emails, scenario, pattern, flags)\n",
    "    results.append({\n",
    "        'Model': model,\n",
    "        'Pattern': name,\n",
    "        'F%': f\"{res['f_pct']:.1f}%\",\n",
    "        'M%': f\"{res['m_pct']:.1f}%\",\n",
    "        'p': f\"{res['p']:.3f}\" if res['p'] >= 0.001 else '<.001',\n",
    "        'd': f\"{res['d']:+.2f}\"\n",
    "    })\n",
    "\n",
    "table2 = pd.DataFrame(results)\n",
    "print(\"Table 2: Gender differences in generated email style (FDR-corrected, n=30 per group)\")\n",
    "print(table2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: S01 Evaluation (Salary Negotiation)\n",
    "\n",
    "No significant gender differences in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s01_settings = [\n",
    "    ('GPT-5.2 \u2192 Gemini 2.0', 'Naturalistic', 'gemini_nat'),\n",
    "    ('GPT-5.2 \u2192 Gemini 2.0', 'Debiased', 'gemini_deb'),\n",
    "    ('Gemini 2.0 \u2192 GPT-5.2', 'Naturalistic', 'gpt52_nat'),\n",
    "    ('Gemini 2.0 \u2192 GPT-5.2', 'Debiased', 'gpt52_deb'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for setting, condition, key in s01_settings:\n",
    "    res = calc_persona_rating(ratings[key], 'S01', 'likelihood_to_grant_raise')\n",
    "    results.append({\n",
    "        'Setting': setting,\n",
    "        'Condition': condition,\n",
    "        'F': f\"{res['f_mean']:.2f}\",\n",
    "        'M': f\"{res['m_mean']:.2f}\",\n",
    "        'Diff': f\"{res['diff']:+.2f}\",\n",
    "        'p': f\"{res['p']:.3f}\"\n",
    "    })\n",
    "\n",
    "table3 = pd.DataFrame(results)\n",
    "print(\"Table 3: S01 Evaluation - Likelihood to grant raise (1-5 scale, n=30 per group)\")\n",
    "print(table3.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 4: S02 Evaluation (Credit Attribution)\n",
    "\n",
    "Significant pro-female bias in most conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s02_settings = [\n",
    "    ('GPT-5.2 \u2192 Gemini 2.0', 'Naturalistic', 'gemini_nat'),\n",
    "    ('GPT-5.2 \u2192 Gemini 2.0', 'Debiased', 'gemini_deb'),\n",
    "    ('GPT-5.2 \u2192 Gemini 2.0', 'Blinded', 'gemini_blind'),\n",
    "    ('Gemini 2.0 \u2192 GPT-5.2', 'Naturalistic', 'gpt52_nat'),\n",
    "    ('Gemini 2.0 \u2192 GPT-5.2', 'Debiased', 'gpt52_deb'),\n",
    "    ('Gemini 2.0 \u2192 GPT-5.2', 'Blinded', 'gpt52_blind'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for setting, condition, key in s02_settings:\n",
    "    res = calc_persona_rating(ratings[key], 'S02', 'likelihood_to_send_correction')\n",
    "    results.append({\n",
    "        'Setting': setting,\n",
    "        'Condition': condition,\n",
    "        'F-M Diff': f\"{res['diff']:+.2f}\",\n",
    "        'p': f\"{res['p']:.3f}\" if res['p'] >= 0.001 else '<.001',\n",
    "        'd': f\"{res['d']:+.2f}\"\n",
    "    })\n",
    "\n",
    "table4 = pd.DataFrame(results)\n",
    "print(\"Table 4: S02 Evaluation - Likelihood to send correction (n=30 per group)\")\n",
    "print(table4.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 5: Blinded Decomposition\n",
    "\n",
    "Decomposing evaluation bias into name-based and style-based components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini evaluator (rating GPT-5.2 emails)\n",
    "gem_nat = calc_persona_rating(ratings['gemini_nat'], 'S02', 'likelihood_to_send_correction')\n",
    "gem_blind = calc_persona_rating(ratings['gemini_blind'], 'S02', 'likelihood_to_send_correction')\n",
    "gem_deb = calc_persona_rating(ratings['gemini_deb'], 'S02', 'likelihood_to_send_correction')\n",
    "\n",
    "# GPT-5.2 evaluator (rating Gemini emails)\n",
    "gpt_nat = calc_persona_rating(ratings['gpt52_nat'], 'S02', 'likelihood_to_send_correction')\n",
    "gpt_blind = calc_persona_rating(ratings['gpt52_blind'], 'S02', 'likelihood_to_send_correction')\n",
    "gpt_deb = calc_persona_rating(ratings['gpt52_deb'], 'S02', 'likelihood_to_send_correction')\n",
    "\n",
    "print(\"Table 5: S02 Bias Decomposition\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGemini 2.0 evaluator:\")\n",
    "print(f\"  Unblinded: {gem_nat['diff']:+.2f} (d={gem_nat['d']:.2f})\")\n",
    "print(f\"  Blinded:   {gem_blind['diff']:+.2f} (d={gem_blind['d']:.2f})\")\n",
    "print(f\"  Debiased:  {gem_deb['diff']:+.2f}\")\n",
    "print(f\"  \u2192 Name component: {gem_nat['diff'] - gem_blind['diff']:+.2f}\")\n",
    "print(f\"  \u2192 Style component: {gem_blind['diff']:+.2f}\")\n",
    "print(f\"  \u2192 Interpretation: Pure style (blinding has no effect)\")\n",
    "\n",
    "print(f\"\\nGPT-5.2 evaluator:\")\n",
    "print(f\"  Unblinded: {gpt_nat['diff']:+.2f} (d={gpt_nat['d']:.2f})\")\n",
    "print(f\"  Blinded:   {gpt_blind['diff']:+.2f} (d={gpt_blind['d']:.2f})\")\n",
    "print(f\"  Debiased:  {gpt_deb['diff']:+.2f}\")\n",
    "print(f\"  \u2192 Name component: {gpt_nat['diff'] - gpt_blind['diff']:+.2f}\")\n",
    "print(f\"  \u2192 Style component: {gpt_blind['diff']:+.2f}\")\n",
    "print(f\"  \u2192 Interpretation: Name + Style (both contribute)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Persona pairs: 30\")\n",
    "print(f\"Total personas: 60\")\n",
    "print(f\"Scenarios: 2\")\n",
    "print(f\"Emails per persona-scenario: 3\")\n",
    "print(f\"Generator models: 2 (GPT-5.2, Gemini 2.0)\")\n",
    "print(f\"Total emails: {len(gpt_emails) + len(gemini_emails)}\")\n",
    "print(f\"Evaluation conditions: 3 (naturalistic, debiased, blinded)\")\n",
    "print(f\"Total ratings: {sum(len(v) for v in ratings.values())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}